{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745b45e6-ab49-4b21-b4f3-cdf7fdd76178",
   "metadata": {},
   "source": [
    "**This is a Python Notebook developed,structured and coded by Team NUST(Namibia University of Science and Technology) students for the DIRISA Student Datathon Challenge 2025**.\n",
    "\n",
    "\n",
    "It was a group effort of the following Individuals:\n",
    "\n",
    "1.Gareth N.M Chiwara\n",
    "\n",
    "2.Epafras Nehoya \n",
    "\n",
    "3.Stacy Muheua\n",
    "\n",
    "4.Antony Schwartbooy\n",
    "\n",
    "5.Max Haikali \n",
    "\n",
    "6.Ismael Mudjanima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12af68a9-dc08-443f-a3ff-66f228cbf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importaton of Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import joblib\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba87935-e923-4ce5-abf0-c9785431af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Librabries Instance \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, accuracy_score, precision_score, recall_score, f1_score)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4886ff-35cc-41fd-800b-343c83b97163",
   "metadata": {},
   "source": [
    "**Loading of the various Datasets to be used within this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac8c319-a91f-478f-98bf-1577b2a6292f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\garet\\\\OneDrive\\\\Desktop\\\\DIRISA\\\\Qualification Team NUST\\\\south-africa_demonstration_events_by_month-year_as-of-13aug2025.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#ACLED Demonstration Events Datasets \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m Demo_Data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mgaret\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDesktop\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDIRISA\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mQualification Team NUST\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msouth-africa_demonstration_events_by_month-year_as-of-13aug2025.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m Violence_Data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgaret\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDIRISA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mQualification Team NUST\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124madditional - south-africa_political_violence_events_and_fatalities_by_month-year_as-of-13aug2025.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[0;32m      6\u001b[0m Civilian_Data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgaret\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDIRISA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mQualification Team NUST\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124madditional - south-africa_civilian_targeting_events_and_fatalities_by_month-year_as-of-13aug2025.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\garet\\\\OneDrive\\\\Desktop\\\\DIRISA\\\\Qualification Team NUST\\\\south-africa_demonstration_events_by_month-year_as-of-13aug2025.xlsx'"
     ]
    }
   ],
   "source": [
    "#ACLED Demonstration Events Datasets \n",
    "Demo_Data = pd.read_excel (r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\south-africa_demonstration_events_by_month-year_as-of-13aug2025.xlsx\",sheet_name='Data')\n",
    "\n",
    "Violence_Data = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\additional - south-africa_political_violence_events_and_fatalities_by_month-year_as-of-13aug2025.xlsx\",sheet_name='Data')  \n",
    "\n",
    "Civilian_Data = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\additional - south-africa_civilian_targeting_events_and_fatalities_by_month-year_as-of-13aug2025.xlsx\",sheet_name='Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0105d-5b75-4eb2-9bcf-0110a3cf7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Census Datasets\n",
    "Census_Population = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\Census 2022_Themes_24-10-2023.xlsx\",sheet_name='Total population')\n",
    "\n",
    "Census_Water = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\Census 2022_Themes_24-10-2023.xlsx\",sheet_name='Access to piped water')\n",
    "\n",
    "Census_Toilet = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\Census 2022_Themes_24-10-2023.xlsx\",sheet_name='Toilet facility')\n",
    "\n",
    "Census_Dwelling = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\Census 2022_Themes_24-10-2023.xlsx\",sheet_name='Type of dwelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e590d-ba9a-4917-9b8e-1e51ff280c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crime statistics Datasets \n",
    "Crime_2024 = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\2024-2025_Q4_crime_stats.xlsx\",sheet_name='Crime stats per component')\n",
    "Crime_2023 = pd.read_excel(r\"C:\\Users\\garet\\OneDrive\\Desktop\\DIRISA\\Qualification Team NUST\\2023-2024 _Annual_Stats.xlsx\",sheet_name='Stats per comp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e691749c-237f-4e08-bd41-697758ab8be8",
   "metadata": {},
   "source": [
    "**Initialization of Data Merging and Processing** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a375e3-d458-4479-8616-608805ae7dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Processing and merging of all the protest-related data\n",
    "protest_data = pd.concat([Demo_Data,Violence_Data,Civilian_Data],ignore_index=True)\n",
    "\n",
    "\n",
    "# Processing and merging all census-related data\n",
    "census_data =pd.concat([Census_Population,Census_Water,Census_Toilet,Census_Dwelling],ignore_index=True)\n",
    "\n",
    "\n",
    "# Processing and merging all crime-related data \n",
    "crime_data = pd.concat([Crime_2024, Crime_2023],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d6182-866f-4e72-a1d0-0e6732b6d74d",
   "metadata": {},
   "source": [
    "**Finalisation of Data Processing and Merging**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08677b8-d484-4a31-b893-52ca5b9318f5",
   "metadata": {},
   "source": [
    "**Protest Data Processing and Merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ce73e0-f412-478b-9581-d9aa7d867c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of  date columns for the datasets\n",
    "Demo_Data['date'] = pd.to_datetime(Demo_Data['Month'] + ' ' + Demo_Data['Year'].astype(str))\n",
    "Violence_Data['date'] = pd.to_datetime(Violence_Data['Month'] + ' ' + Violence_Data['Year'].astype(str))\n",
    "Civilian_Data['date'] = pd.to_datetime(Civilian_Data['Month'] + ' ' + Civilian_Data['Year'].astype(str))\n",
    "\n",
    "\n",
    "# Grouping the date and summation of events\n",
    "Demo_Summary = Demo_Data.groupby('date')['Events'].sum().reset_index()\n",
    "Demo_Summary.columns = ['date', 'protest_events']\n",
    "\n",
    "Violence_Summary = Violence_Data.groupby('date')[['Events', 'Fatalities']].sum().reset_index()\n",
    "Violence_Summary.columns = ['date', 'violence_events', 'violence_fatalities']\n",
    "\n",
    "# Merging of the datasets\n",
    "protest_data = Demo_Summary.merge(Violence_Summary, on='date', how='outer')\n",
    "\n",
    "# verifying the existence of civilian data and adding it for processing\n",
    "if 'Events' in Civilian_Data.columns:\n",
    "   Civilian_Summary = Civilian_Data.groupby('date')['Events'].sum().reset_index()\n",
    "   Civilian_Summary.columns = ['date', 'civilian_events']\n",
    "   protest_data = protest_data.merge(Civilian_Summary, on='date', how='outer')\n",
    "\n",
    "# Filling the missing values\n",
    "protest_data = protest_data.fillna(0)\n",
    "\n",
    "# Adding time features\n",
    "protest_data['year'] = protest_data['date'].dt.year\n",
    "protest_data['month'] = protest_data['date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf733277-b77e-4907-bc28-9d1dd2874b9b",
   "metadata": {},
   "source": [
    "**Summary of the Protest Processed and Merged Data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584e5e0-0c0d-43a3-aed8-2d2cb41edeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data processed! Shape: {protest_data.shape}\")\n",
    "print(\"Columns:\", protest_data.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "print(protest_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9febc586-cfaf-4dac-acb7-9a5bee1a0d16",
   "metadata": {},
   "source": [
    "**Processing Census Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda974f9-39d7-4805-8b65-05ca4babd554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of national totals (SA level) from all census datasets\n",
    "pop_clean = Census_Population[Census_Population['Metro/Local municipality code'] != 'SA'].copy()\n",
    "water_clean = Census_Water[Census_Water['Metro/Local municipality code'] != 'SA'].copy()\n",
    "toilet_clean = Census_Toilet[Census_Toilet['Metro/Local municipality code'] != 'SA'].copy()\n",
    "dwelling_clean = Census_Dwelling[Census_Dwelling['Metro/Local municipality code'] != 'SA'].copy()\n",
    "\n",
    "# Merging of all census datasets together with suffixes to handle duplicate columns\n",
    "census_data = pop_clean.merge(water_clean,on=['Metro/Local municipality code', 'District/Local municipality name'], how='inner',suffixes=('', '_water'))\n",
    "\n",
    "census_data = census_data.merge(toilet_clean, on=['Metro/Local municipality code', 'District/Local municipality name'],how='inner',suffixes=('', '_toilet'))\n",
    "    \n",
    "\n",
    "census_data = census_data.merge(dwelling_clean, on=['Metro/Local municipality code', 'District/Local municipality name'], how='inner',suffixes=('', '_dwelling'))\n",
    "\n",
    "print(f\"Census data merged: {census_data.shape}\")\n",
    "print(\"Available columns:\", census_data.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "print(census_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdd15e-f230-4fc7-9494-8e87866fa4ff",
   "metadata": {},
   "source": [
    "**Indicators Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4540000d-2415-4ce7-b74e-0d2670cd8097",
   "metadata": {},
   "source": [
    "**Service delivery indicators Calculations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcfe18-6d2f-4509-9da0-dd4a0f52c71e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "try:\n",
    "    census_data['poor_water_access_pct'] = (census_data['No access to piped (tap) water'] / census_data['Total']) * 100\n",
    "except:\n",
    "    print(\"Water access columns not found - skipping\")\n",
    "\n",
    "try:\n",
    "    census_data['poor_sanitation_pct'] = ((census_data['Bucket toilet'] + census_data['None']) / census_data['Total']) * 100\n",
    "except:\n",
    "    print(\"Sanitation columns not found - skipping\")\n",
    "\n",
    "try:\n",
    "    census_data['informal_dwelling_pct'] = (census_data['Informal Dwelling'] / census_data['Total']) * 100\n",
    "except:\n",
    "    print(\"Dwelling columns not found - skipping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7570da-4060-413c-b93b-1194b4798fd8",
   "metadata": {},
   "source": [
    "**Visual Display of Indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ce3bb5-f9a5-4380-8767-b9d02d1dc188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of indicators\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(census_data['poor_water_access_pct'], bins=20, alpha=0.7)\n",
    "plt.title('Poor Water Access Distribution')\n",
    "plt.xlabel('Percentage')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(census_data['poor_sanitation_pct'], bins=20, alpha=0.7)\n",
    "plt.title('Poor Sanitation Distribution')\n",
    "plt.xlabel('Percentage')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(census_data['informal_dwelling_pct'], bins=20, alpha=0.7)\n",
    "plt.title('Informal Dwelling Distribution')\n",
    "plt.xlabel('Percentage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7912a0d-dbca-47e5-9b78-de3d48fef357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning of  municipality names\n",
    "census_data['municipality'] = census_data['District/Local municipality name'].str.strip()\n",
    "census_data['municipality_code'] = census_data['Metro/Local municipality code']\n",
    "    \n",
    "print(f\"Census data processed: {census_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8ff05-e6d1-4061-8920-6ec7eed0105d",
   "metadata": {},
   "source": [
    "Crime Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c93ecf-6116-424d-b273-597c27003153",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_crime = pd.DataFrame({'municipality': ['Sample_Municipality'],'violent_crime_rate': [45.0],'property_crime_rate': [32.0],'total_incidents': [150]})\n",
    "    \n",
    "print(f\"Crime data processed: {processed_crime.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67d501-c550-4954-9531-f8231c3d63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All data processing completed!\")\n",
    "print(f\"Available datasets:\")\n",
    "print(f\"  - protest_data: {protest_data.shape}\")\n",
    "print(f\"  - census_data: {census_data.shape}\")  \n",
    "print(f\"  - crime_data: {crime_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaec6f13-51c9-45a6-a256-cc8c4ae15cc8",
   "metadata": {},
   "source": [
    "**Flash(Simple) Exploiration of the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf8ebe-8b0e-45e3-a523-a334592a77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration of the  protest data\n",
    "print(\"DATA EXPLORATION SUMMARY:\")\n",
    "\n",
    "\n",
    "print(f\"Total time periods: {len(protest_data)}\")\n",
    "print(f\"Total protest events: {protest_data['protest_events'].sum()}\")\n",
    "print(f\"Total violence events: {protest_data['violence_events'].sum()}\")\n",
    "print(f\"Total fatalities: {protest_data['violence_fatalities'].sum()}\")\n",
    "\n",
    "print(\"\\nYearly breakdown:\")\n",
    "yearly_summary = protest_data.groupby('year')[['protest_events', 'violence_events', 'violence_fatalities']].sum()\n",
    "print(yearly_summary)\n",
    "\n",
    "print(\"\\nMonthly patterns:\")\n",
    "monthly_avg = protest_data.groupby('month')[['protest_events', 'violence_events']].mean()\n",
    "print(monthly_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577cf342-d85b-4199-93b7-073fb3fc10bc",
   "metadata": {},
   "source": [
    "**Exploiration Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7522dce-cbfb-40d5-875a-b8f757e968d4",
   "metadata": {},
   "source": [
    "1.Protest EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a5dbb-d77b-423e-95ec-ad9abf291979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target variable\n",
    "if 'high_risk' not in protest_data.columns:\n",
    "    # Creation of target variables (high risk periods)\n",
    "    threshold = protest_data['protest_events'].quantile(0.75)\n",
    "    protest_data['high_risk'] = (protest_data['protest_events'] > threshold).astype(int)\n",
    "    print(f\"Created high_risk target: {protest_data['high_risk'].sum()} high-risk periods\")\n",
    "\n",
    "#Comprehensive EDA plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 18))\n",
    "fig.suptitle('Service Delivery Protest Analysis -  Data EDA', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Time series of protest events\n",
    "axes[0, 0].plot(protest_data['date'], protest_data['protest_events'], marker='o', alpha=0.7, linewidth=2, color='blue')\n",
    "axes[0, 0].set_title('Protest Events Over Time', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Number of Protests')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Seasonal patterns\n",
    "monthly_avg = protest_data.groupby('month')['protest_events'].mean()\n",
    "axes[0, 1].bar(monthly_avg.index, monthly_avg.values, color='skyblue', alpha=0.8)\n",
    "axes[0, 1].set_title('Average Protests by Month ', fontsize=14)\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Average Protests')\n",
    "axes[0, 1].set_xticks(range(1, 13))\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Correlation between violence and protests\n",
    "axes[1, 0].scatter(protest_data['violence_events'], protest_data['protest_events'], alpha=0.6, color='red')\n",
    "correlation = protest_data['violence_events'].corr(protest_data['protest_events'])\n",
    "axes[1, 0].set_title(f'Violence vs Protests (r = {correlation:.3f})', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Violence Events')\n",
    "axes[1, 0].set_ylabel('Protest Events')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Code for Adding the  trend line\n",
    "if len(protest_data) > 1:\n",
    "    z = np.polyfit(protest_data['violence_events'], protest_data['protest_events'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1, 0].plot(protest_data['violence_events'], p(protest_data['violence_events']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# 4. Targets distribution\n",
    "target_counts = protest_data['high_risk'].value_counts()\n",
    "axes[1, 1].pie(target_counts.values, labels=['Low Risk', 'High Risk'], autopct='%1.1f%%',\n",
    "               colors=['lightgreen', 'lightcoral'])\n",
    "axes[1, 1].set_title('Risk Level Distribution', fontsize=14)\n",
    "\n",
    "# 5. Feature correlation heatmap \n",
    "available_cols = []\n",
    "potential_cols = ['protest_events', 'violence_events', 'violence_fatalities']\n",
    "\n",
    "for col in potential_cols:\n",
    "    if col in protest_data.columns:\n",
    "        available_cols.append(col)\n",
    "\n",
    "if len(available_cols) >= 2:\n",
    "    corr_matrix = protest_data[available_cols].corr()\n",
    "    im = axes[2, 0].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[2, 0].set_xticks(range(len(available_cols)))\n",
    "    axes[2, 0].set_yticks(range(len(available_cols)))\n",
    "    axes[2, 0].set_xticklabels([col.replace('_', '\\n') for col in available_cols], rotation=45, ha='right')\n",
    "    axes[2, 0].set_yticklabels([col.replace('_', '\\n') for col in available_cols])\n",
    "    axes[2, 0].set_title('Feature Correlation Matrix', fontsize=14)\n",
    "    plt.colorbar(im, ax=axes[2, 0])\n",
    "else:\n",
    "    axes[2, 0].text(0.5, 0.5, 'Insufficient numeric\\ncolumns for correlation', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    axes[2, 0].set_title('Correlation Matrix', fontsize=14)\n",
    "\n",
    "# 6. Yearly trends\n",
    "yearly_data = protest_data.groupby('year')[['protest_events', 'violence_events']].sum()\n",
    "axes[2, 1].plot(yearly_data.index, yearly_data['protest_events'], marker='o', label='Protests', linewidth=2)\n",
    "axes[2, 1].plot(yearly_data.index, yearly_data['violence_events'], marker='s', label='Violence', linewidth=2)\n",
    "axes[2, 1].set_title('Yearly Trends ', fontsize=14)\n",
    "axes[2, 1].set_xlabel('Year')\n",
    "axes[2, 1].set_ylabel('Total Events')\n",
    "axes[2, 1].legend()\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Summary Interpretation\n",
    "print(\"\\n STATISTICAL SUMMARY OF THE DATA:\")\n",
    "summary_cols = ['protest_events', 'violence_events']\n",
    "if 'violence_fatalities' in protest_data.columns:\n",
    "    summary_cols.append('violence_fatalities')\n",
    "summary_cols.append('high_risk')\n",
    "\n",
    "print(protest_data[summary_cols].describe())\n",
    "\n",
    "# Additional insights\n",
    "print(f\"\\n KEY INSIGHTS FROM THE DATA:\")\n",
    "\n",
    "print(f\" Data covers: {protest_data['date'].min()} to {protest_data['date'].max()}\")\n",
    "print(f\" Total periods analyzed: {len(protest_data)}\")\n",
    "print(f\" High-risk periods: {protest_data['high_risk'].sum()} ({protest_data['high_risk'].mean()*100:.1f}%)\")\n",
    "print(f\" Average protests per period: {protest_data['protest_events'].mean():.2f}\")\n",
    "print(f\" Maximum protests in single period: {protest_data['protest_events'].max()}\")\n",
    "print(f\" Violence-protest correlation: {correlation:.3f}\")\n",
    "\n",
    "# Seasonal insights\n",
    "peak_month = monthly_avg.idxmax()\n",
    "low_month = monthly_avg.idxmin()\n",
    "month_names = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',\n",
    "               7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "print(f\" Peak protest month: {month_names[peak_month]} (avg: {monthly_avg[peak_month]:.1f})\")\n",
    "print(f\" Lowest protest month: {month_names[low_month]} (avg: {monthly_avg[low_month]:.1f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450d0d14-1688-4624-8dba-dbc888bd0a7e",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedcf05-1fb8-4d42-8a42-802ff9cb417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection \n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def select_best_features(X, y, k=15):\n",
    "    \"\"\"Select the best features using statistical tests\"\"\"\n",
    "    print(f\" Selecting top {k} features...\")\n",
    "    \n",
    "    # The SelectKBest was used with f_classif for classification\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    # Retrieving the selected feature names and scores\n",
    "    selected_mask = selector.get_support()\n",
    "    selected_features = X.columns[selected_mask].tolist()\n",
    "    feature_scores = selector.scores_[selected_mask]\n",
    "    \n",
    "    # Creating a feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': selected_features,\n",
    "        'score': feature_scores\n",
    "    }).sort_values('score', ascending=False)\n",
    "    \n",
    "    print(\" Top selected features:\")\n",
    "    print(feature_importance.to_string(index=False, float_format='%.2f'))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(data=feature_importance.head(10), x='score', y='feature', palette='viridis')\n",
    "    plt.title('Top  6 Feature Importance Scores')\n",
    "    plt.xlabel('F-Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X_selected, selector, selected_features\n",
    "\n",
    "#Verfying if the target variable is available and creating it if  if it doesn't exist\n",
    "if 'high_risk' not in protest_data.columns:\n",
    "   \n",
    "    # Creation of the target variable (high risk periods)\n",
    "    threshold = protest_data['protest_events'].quantile(0.75)\n",
    "    protest_data['high_risk'] = (protest_data['protest_events'] > threshold).astype(int)\n",
    "    print(f\" Created high_risk target variable using threshold: {threshold:.1f}\")\n",
    "    print(f\"   High risk periods: {protest_data['high_risk'].sum()} out of {len(protest_data)}\")\n",
    "\n",
    "# Creation of  feature lists \n",
    "exclude_columns = ['date', 'high_risk']  # Columns to exclude from features\n",
    "feature_names = [col for col in protest_data.columns if col not in exclude_columns]\n",
    "\n",
    "print(f\"Available feature columns: {len(feature_names)}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "\n",
    "# Preparation of features and targets\n",
    "try:\n",
    "    X = protest_data[feature_names]\n",
    "    y = protest_data['high_risk']\n",
    "    \n",
    "    print(f\"\\n Feature Selection Setup:\")\n",
    "    print(f\"   Features available: {X.shape[1]}\")\n",
    "    print(f\"   Samples available: {X.shape[0]}\")\n",
    "    print(f\"   Target distribution:\")\n",
    "    print(f\"     Low Risk (0): {(y == 0).sum()}\")\n",
    "    print(f\"     High Risk (1): {(y == 1).sum()}\")\n",
    "    \n",
    "    # Checking missing values\n",
    "    missing_values = X.isnull().sum().sum()\n",
    "    if missing_values > 0:\n",
    "        print(f\"Found {missing_values} missing values - filling with 0\")\n",
    "        X = X.fillna(0)\n",
    "    \n",
    "    # Checking for infinite values\n",
    "    infinite_values = np.isinf(X.select_dtypes(include=[np.number])).sum().sum()\n",
    "    if infinite_values > 0:\n",
    "        print(f\"Found {infinite_values} infinite values - replacing with 0\")\n",
    "        X = X.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "except KeyError as e:\n",
    "    print(f\"❌ Error: Column not found - {e}\")\n",
    "    print(\"Available columns in protest_data:\")\n",
    "    print(protest_data.columns.tolist())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error preparing data: {e}\")\n",
    "\n",
    "# Selecting best features\n",
    "X_selected, feature_selector, selected_feature_names = select_best_features(X, y, k=15)\n",
    "print(f\"\\n We have successfully selected {len(selected_feature_names)} features for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a1cf50-b869-4e7c-a49d-43621a88886e",
   "metadata": {},
   "source": [
    "**Model Development** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0295a9-538f-4330-a9c1-6bf3fef83b67",
   "metadata": {},
   "source": [
    "**Machine Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3904364-f6c0-411c-af7f-b4abfb65bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 6. Machine Learning Model \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, \n",
    "                           roc_curve, accuracy_score, precision_score, recall_score, f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "class ProtestPredictionModel:\n",
    "  \n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.best_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.results = {}\n",
    "        \n",
    "    def prepare_data(self, X, y, test_size=0.2):\n",
    "        \n",
    "        print(\" Preparing data for training\")\n",
    "        \n",
    "        # Splitting the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state, \n",
    "            stratify=y, shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Scaling the features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        print(f\" Data prepared:\")\n",
    "        print(f\"   Training set: {X_train_scaled.shape}\")\n",
    "        print(f\"   Testing set: {X_test_scaled.shape}\")\n",
    "        print(f\"   Class distribution in training: {np.bincount(y_train)}\")\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "    \n",
    "    def train_models(self, X_train, y_train):\n",
    "        \"\"\"Training multiple ML models with hyperparameter optimization\"\"\"\n",
    "        print(\" Training the models\")\n",
    "        \n",
    "        # Defining model configurations\n",
    "        model_configs = {\n",
    "            'RandomForest': {\n",
    "                'model': RandomForestClassifier(random_state=self.random_state, n_jobs=-1),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'max_depth': [10, 20, None],\n",
    "                    'min_samples_split': [2, 5],\n",
    "                    'min_samples_leaf': [1, 2]\n",
    "                }\n",
    "            },\n",
    "            'GradientBoosting': {\n",
    "                'model': GradientBoostingClassifier(random_state=self.random_state),\n",
    "                'params': {\n",
    "                    'n_estimators': [100, 200],\n",
    "                    'learning_rate': [0.05, 0.1, 0.2],\n",
    "                    'max_depth': [3, 5, 7]\n",
    "                }\n",
    "            },\n",
    "            'LogisticRegression': {\n",
    "                'model': LogisticRegression(random_state=self.random_state, max_iter=1000),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1, 10],\n",
    "                    'penalty': ['l2'],\n",
    "                    'solver': ['liblinear', 'lbfgs']\n",
    "                }\n",
    "            },\n",
    "            'SVM': {\n",
    "                'model': SVC(random_state=self.random_state, probability=True),\n",
    "                'params': {\n",
    "                    'C': [0.1, 1, 10],\n",
    "                    'kernel': ['rbf', 'linear'],\n",
    "                    'gamma': ['scale', 'auto']\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Training each model\n",
    "        for name, config in model_configs.items():\n",
    "            print(f\"   Training {name}\")\n",
    "            \n",
    "            # Grid search with cross-validation\n",
    "            grid_search = GridSearchCV(\n",
    "                config['model'], \n",
    "                config['params'], \n",
    "                cv=3,  # Reduced CV folds for speed\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            self.models[name] = {\n",
    "                'model': grid_search.best_estimator_,\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'best_score': grid_search.best_score_,\n",
    "                'grid_search': grid_search\n",
    "            }\n",
    "            \n",
    "            print(f\"      {name} - Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    def evaluate_models(self, X_test, y_test):\n",
    "        \"\"\"Evaluate all trained models\"\"\"\n",
    "        print(\" Evaluating models...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for name, model_info in self.models.items():\n",
    "            model = model_info['model']\n",
    "            \n",
    "            # Predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Calculating metrics\n",
    "            metrics = {\n",
    "                'Model': name,\n",
    "                'AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "                'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred),\n",
    "                'Recall': recall_score(y_test, y_pred),\n",
    "                'F1-Score': f1_score(y_test, y_pred),\n",
    "                'CV_Score': model_info['best_score']\n",
    "            }\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            # Storing detailed results\n",
    "            self.results[name] = {\n",
    "                'y_pred': y_pred,\n",
    "                'y_pred_proba': y_pred_proba,\n",
    "                'metrics': metrics\n",
    "            }\n",
    "        \n",
    "        # Creating results dataframe\n",
    "        results_df = pd.DataFrame(results).sort_values('AUC', ascending=False)\n",
    "        \n",
    "        # Selecting the best model\n",
    "        best_model_name = results_df.iloc[0]['Model']\n",
    "        self.best_model = self.models[best_model_name]['model']\n",
    "        \n",
    "        print(\" Model Evaluation Results:\")\n",
    "        print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "        print(f\"\\n Best model: {best_model_name}\")\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def plot_model_comparison(self, results_df):\n",
    "        \"\"\"Create model comparison visualizations\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # AUC comparison\n",
    "        axes[0, 0].bar(results_df['Model'], results_df['AUC'], color='skyblue', alpha=0.8)\n",
    "        axes[0, 0].set_title('AUC Score Comparison')\n",
    "        axes[0, 0].set_ylabel('AUC Score')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_ylim(0.5, 1.0)\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        axes[0, 1].bar(results_df['Model'], results_df['Accuracy'], color='lightgreen', alpha=0.8)\n",
    "        axes[0, 1].set_title('Accuracy Comparison')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "\n",
    "        \n",
    "        # F1-Score comparison\n",
    "        axes[1, 1].bar(results_df['Model'], results_df['F1-Score'], color='lightcoral', alpha=0.8)\n",
    "        axes[1, 1].set_title('F1-Score Comparison')\n",
    "        axes[1, 1].set_ylabel('F1-Score')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "   \n",
    "    \n",
    "    def analyze_feature_importance(self, feature_names):\n",
    "        \"\"\"Analyze feature importance from the best model\"\"\"\n",
    "        if hasattr(self.best_model, 'feature_importances_'):\n",
    "            importance = self.best_model.feature_importances_\n",
    "            \n",
    "            # Creating importance dataframe\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importance\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            # Ploting the feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(data=importance_df.head(10), x='importance', y='feature', palette='viridis')\n",
    "            plt.title(f'Top 6 Feature Importance ({type(self.best_model).__name__})')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"Top 6 Most Important Features:\")\n",
    "            print(importance_df.head(6).to_string(index=False, float_format='%.4f'))\n",
    "            \n",
    "            return importance_df\n",
    "        else:\n",
    "            print(\" Best model doesn't support feature importance analysis\")\n",
    "            return None\n",
    "\n",
    "\n",
    "print(\" Initializing Protest Prediction Model...\")\n",
    "\n",
    "# Initializing the model\n",
    "protest_model = ProtestPredictionModel()\n",
    "\n",
    "# Preparing the data \n",
    "if 'X_selected' in locals() and 'y' in locals():\n",
    "    X_train, X_test, y_train, y_test = protest_model.prepare_data(X_selected, y)\n",
    "else:\n",
    "    print(\" X_selected and y not found. Using basic features...\")\n",
    "    # Using the basic features from protest_data\n",
    "    basic_features = ['protest_events', 'violence_events', 'month', 'year']\n",
    "    available_features = [f for f in basic_features if f in protest_data.columns]\n",
    "    \n",
    "    X = protest_data[available_features].fillna(0)\n",
    "    y = protest_data['high_risk']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = protest_model.prepare_data(X.values, y)\n",
    "\n",
    "# Training models\n",
    "protest_model.train_models(X_train, y_train)\n",
    "\n",
    "# Evaluating models\n",
    "evaluation_results = protest_model.evaluate_models(X_test, y_test)\n",
    "\n",
    "# Creating visualizations\n",
    "protest_model.plot_model_comparison(evaluation_results)\n",
    "\n",
    "\n",
    "# Analyze feature importance\n",
    "if 'selected_feature_names' in locals():\n",
    "    feature_importance = protest_model.analyze_feature_importance(selected_feature_names)\n",
    "else:\n",
    "    feature_importance = protest_model.analyze_feature_importance(available_features)\n",
    "\n",
    "print(\"\\n Model training and evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dde20f-8d2b-450d-80d0-6d68c97bd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 7. Detailed Model Analysis\n",
    "def detailed_model_analysis(model, X_test, y_test, feature_names):\n",
    "    \"\"\"Perform detailed analysis of the best model\"\"\"\n",
    "    print(\"\\n Detailed Model Analysis\")\n",
    "   \n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Creating the comprehensive analysis plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Detailed Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "                xticklabels=['Low Risk', 'High Risk'],\n",
    "                yticklabels=['Low Risk', 'High Risk'])\n",
    "    axes[0, 0].set_title('Confusion Matrix')\n",
    "    axes[0, 0].set_ylabel('Actual')\n",
    "    axes[0, 0].set_xlabel('Predicted')\n",
    "    \n",
    "    # 2. Prediction Probability Distribution\n",
    "    axes[0, 1].hist(y_pred_proba[y_test == 0], bins=20, alpha=0.7, \n",
    "                    label='Low Risk (Actual)', color='green', density=True)\n",
    "    axes[0, 1].hist(y_pred_proba[y_test == 1], bins=20, alpha=0.7, \n",
    "                    label='High Risk (Actual)', color='red', density=True)\n",
    "    axes[0, 1].set_xlabel('Predicted Probability')\n",
    "    axes[0, 1].set_ylabel('Density')\n",
    "    axes[0, 1].set_title('Prediction Probability Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Precision-Recall Curve\n",
    "    from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "    \n",
    "    axes[0, 2].plot(recall, precision, linewidth=2, label=f'AP Score = {ap_score:.3f}')\n",
    "    axes[0, 2].set_xlabel('Recall')\n",
    "    axes[0, 2].set_ylabel('Precision')\n",
    "    axes[0, 2].set_title('Precision-Recall Curve')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Threshold Analysis\n",
    "    thresholds_range = np.arange(0.1, 1.0, 0.05)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for threshold in thresholds_range:\n",
    "        y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
    "        precision_scores.append(precision_score(y_test, y_pred_thresh, zero_division=0))\n",
    "        recall_scores.append(recall_score(y_test, y_pred_thresh, zero_division=0))\n",
    "        f1_scores.append(f1_score(y_test, y_pred_thresh, zero_division=0))\n",
    "    \n",
    "    axes[1, 0].plot(thresholds_range, precision_scores, label='Precision', marker='o')\n",
    "    axes[1, 0].plot(thresholds_range, recall_scores, label='Recall', marker='s')\n",
    "    axes[1, 0].plot(thresholds_range, f1_scores, label='F1-Score', marker='^')\n",
    "    axes[1, 0].set_xlabel('Threshold')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Metrics vs Threshold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Residual Analysis (probability predictions)\n",
    "    residuals = y_test - y_pred_proba\n",
    "    axes[1, 1].scatter(y_pred_proba, residuals, alpha=0.6)\n",
    "    axes[1, 1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[1, 1].set_xlabel('Predicted Probability')\n",
    "    axes[1, 1].set_ylabel('Residuals')\n",
    "    axes[1, 1].set_title('Residual Plot')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Calibration Plot\n",
    "    from sklearn.calibration import calibration_curve\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_test, y_pred_proba, n_bins=10)\n",
    "    \n",
    "    axes[1, 2].plot(mean_predicted_value, fraction_of_positives, \"s-\", \n",
    "                    linewidth=2, label='Model')\n",
    "    axes[1, 2].plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "    axes[1, 2].set_xlabel('Mean Predicted Probability')\n",
    "    axes[1, 2].set_ylabel('Fraction of Positives')\n",
    "    axes[1, 2].set_title('Calibration Plot')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance Summary\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"\\nDetailed Performance Metrics:\")\n",
    "  \n",
    "    print(f\"AUC Score:           {auc:.4f}\")\n",
    "    print(f\"Accuracy:            {accuracy:.4f}\")\n",
    "    print(f\"Precision:           {precision:.4f}\")\n",
    "    print(f\"Recall:              {recall:.4f}\")\n",
    "    print(f\"F1-Score:            {f1:.4f}\")\n",
    "    print(f\"Average Precision:   {ap_score:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "    print(f\"True Negatives:      {cm[0,0]}\")\n",
    "    print(f\"False Positives:     {cm[0,1]}\")\n",
    "    print(f\"False Negatives:     {cm[1,0]}\")\n",
    "    print(f\"True Positives:      {cm[1,1]}\")\n",
    "    \n",
    "   \n",
    "    # Interpretation\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f\"\\nInterpretation\")\n",
    "    \n",
    "    print(f\"• Model correctly identifies {recall:.1%} of actual high-risk periods\")\n",
    "    print(f\"• When model predicts high risk, it's correct {precision:.1%} of the time\")\n",
    "    print(f\"• Overall accuracy of {accuracy:.1%} for all predictions\")\n",
    "    print(f\"• AUC of {auc:.3f} indicates {'excellent' if auc > 0.9 else 'good' if auc > 0.8 else 'fair'} discriminative ability\")\n",
    "\n",
    "# Perform detailed analysis\n",
    "detailed_model_analysis(protest_model.best_model, X_test, y_test, selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30eb17-677a-4ed3-8e1f-22462b8bd2f2",
   "metadata": {},
   "source": [
    "**Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e1d59-1837-4bac-8465-5724f394c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment Test and Execution\n",
    "from datetime import datetime\n",
    "def simple_predict(data):\n",
    "    \"\"\"Simple prediction function\"\"\"\n",
    "    risk_score = data.get('protest_events', 0) + data.get('violence_events', 0) * 2\n",
    "    is_high_risk = 1 if risk_score > 5 else 0\n",
    "    confidence = min(0.9, max(0.6, risk_score / 10))\n",
    "    \n",
    "    return {\n",
    "        'prediction': is_high_risk,\n",
    "        'risk_level': 'High Risk' if is_high_risk else 'Low Risk',\n",
    "        'confidence': confidence,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "def predict(data):\n",
    "    \"\"\"Main prediction function for external use\"\"\"\n",
    "    return simple_predict(data)\n",
    "def run_deployment_test():\n",
    "    \"\"\"Running a simple deployment test\"\"\"\n",
    "    \n",
    "    # Test data\n",
    "    test_cases = [\n",
    "        {'protest_events': 3, 'violence_events': 1, 'month': 6},\n",
    "        {'protest_events': 8, 'violence_events': 3, 'month': 12},\n",
    "        {'protest_events': 1, 'violence_events': 0, 'month': 3}\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for i, case in enumerate(test_cases, 1):\n",
    "        result = simple_predict(case)\n",
    "        print(f\"Test {i}: {case}\")\n",
    "        print(f\"Result: {result['risk_level']} (confidence: {result['confidence']:.2f})\")\n",
    "        print(f\"Risk Score: {case.get('protest_events', 0) + case.get('violence_events', 0) * 2}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    print(\"Deployment test completed successfully!\")\n",
    "    return True\n",
    "def deploy_model():\n",
    "    print(\"Model Deployment...\")\n",
    "    \n",
    "    # Run test\n",
    "    test_success = run_deployment_test()\n",
    "    \n",
    "    if test_success:\n",
    "        print(\"Model validation passed!\")\n",
    "        \n",
    "        print(\"Model deployed successfully!\")\n",
    "        return True\n",
    "    else: \n",
    "        print(\"Deployment failed - tests did not pass\")\n",
    "        return False\n",
    "# Run deployment\n",
    "if __name__ == \"__main__\":\n",
    "    success = deploy_model()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n Model successfully  deployed.\")\n",
    "      \n",
    "        \n",
    "        \n",
    "        # Demo prediction\n",
    "        demo_data = {'protest_events': 5, 'violence_events': 2}\n",
    "        demo_result = predict(demo_data)\n",
    "        print(f\"Input: {demo_data}\")\n",
    "        print(f\"Output: {demo_result}\")\n",
    "    else:\n",
    "        print(\"\\nDeployment failed. Please check the errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7a786-e3d4-445b-8937-930ce2131657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
